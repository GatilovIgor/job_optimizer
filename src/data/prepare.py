import pandas as pd
import numpy as np
import pathlib
import re
import os
from html.parser import HTMLParser

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –ü–£–¢–ï–ô ---
# –°–∫—Ä–∏–ø—Ç –ª–µ–∂–∏—Ç –≤ src/data/, –ø–æ–¥–Ω–∏–º–∞–µ–º—Å—è –Ω–∞ 2 —É—Ä–æ–≤–Ω—è –≤–≤–µ—Ä—Ö –∫ –∫–æ—Ä–Ω—é –ø—Ä–æ–µ–∫—Ç–∞
CURRENT_DIR = pathlib.Path(__file__).resolve().parent
ROOT_DIR = CURRENT_DIR.parent.parent
DATA_DIR = ROOT_DIR / "data"


class MLStripper(HTMLParser):
    def __init__(self):
        super().__init__()
        self.reset()
        self.strict = False
        self.convert_charrefs = True
        self.text = []

    def handle_data(self, d):
        self.text.append(d)

    def get_data(self):
        return "".join(self.text)


def strip_tags(html_txt):
    """–£–¥–∞–ª—è–µ—Ç HTML —Ç–µ–≥–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞."""
    if not isinstance(html_txt, str): return ""
    try:
        s = MLStripper()
        s.feed(html_txt)
        return " ".join(s.get_data().split())
    except:
        return html_txt


def parse_pg_array(array_str):
    """–ü–∞—Ä—Å–∏—Ç –º–∞—Å—Å–∏–≤ –∏–∑ PostgreSQL –≤–∏–¥–∞ {1,2,3} –≤ —Å–ø–∏—Å–æ–∫ python."""
    if pd.isna(array_str) or str(array_str) == '{}': return []
    # –£–¥–∞–ª—è–µ–º —Ñ–∏–≥—É—Ä–Ω—ã–µ —Å–∫–æ–±–∫–∏ –∏ —Ä–∞–∑–±–∏–≤–∞–µ–º
    content = str(array_str).strip('{}')
    if not content: return []
    return [int(x) for x in content.split(',') if x.strip().isdigit()]


def clean_skill_name(raw_name):
    """–û—á–∏—â–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –Ω–∞–≤—ã–∫–∞ –æ—Ç –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –≤–∏–¥–∞ Keyskill': ['..."""
    if pd.isna(raw_name): return ""
    txt = str(raw_name)
    # –£–±–∏—Ä–∞–µ–º –Ω–∞—á–∞–ª–æ —Å—Ç—Ä–æ–∫–∏
    txt = txt.replace("Keyskill': ['", "")
    # –£–±–∏—Ä–∞–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ —Ö–≤–æ—Å—Ç—ã (–µ—Å–ª–∏ –µ—Å—Ç—å –∑–∞–∫—Ä—ã–≤–∞—é—â–∏–µ —Å–∫–æ–±–∫–∏)
    txt = txt.replace("']", "")
    return txt.strip()


def load_skills(skills_path):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ —á–∏—Å—Ç–∏—Ç —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫ –Ω–∞–≤—ã–∫–æ–≤."""
    if not skills_path.exists():
        print(f"‚ö†Ô∏è –§–∞–π–ª –Ω–∞–≤—ã–∫–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω: {skills_path}")
        return {}

    print(f"üìñ –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞–≤—ã–∫–æ–≤ –∏–∑ {skills_path.name}...")
    df_skills = pd.read_csv(skills_path)

    # –ü—Ä–∏–º–µ–Ω—è–µ–º –æ—á–∏—Å—Ç–∫—É –∏–º–µ–Ω
    df_skills['name_clean'] = df_skills['name'].apply(clean_skill_name)

    # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å id -> name
    return dict(zip(df_skills['skill_id'], df_skills['name_clean']))


def load_merged_data(data_dir):
    """–ò—â–µ—Ç CSV —Ñ–∞–π–ª—ã —Å –¥–∞–Ω–Ω—ã–º–∏ (–∏—Å–∫–ª—é—á–∞—è skills.csv) –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç –∏—Ö."""
    all_files = list(data_dir.glob("*.csv"))
    # –ò—Å–∫–ª—é—á–∞–µ–º skills.csv –∏ —Ñ–∞–π–ª—ã —ç–∫—Å–ø–æ—Ä—Ç–∞, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å, –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —á–∞–Ω–∫–∏ –¥–∞–Ω–Ω—ã—Ö
    data_files = [f for f in all_files if f.name != 'skills.csv' and 'vacancies_processed' not in f.name]

    if not data_files:
        print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–∞–Ω–Ω—ã—Ö (CSV) –≤ –ø–∞–ø–∫–µ data!")
        return pd.DataFrame()

    print(f"üì¶ –ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ –¥–∞–Ω–Ω—ã—Ö: {len(data_files)} {[f.name for f in data_files]}")

    dfs = []
    for f in data_files:
        print(f"   + –ß—Ç–µ–Ω–∏–µ {f.name}...")
        try:
            # low_memory=False –ø–æ–º–æ–≥–∞–µ—Ç, –µ—Å–ª–∏ —Ç–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö —Å–º–µ—à–∞–Ω—ã
            df_chunk = pd.read_csv(f, low_memory=False)
            dfs.append(df_chunk)
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {f.name}: {e}")

    if not dfs:
        return pd.DataFrame()

    return pd.concat(dfs, ignore_index=True)


def main():
    print("üöÄ STEP: Data Preparation (Merged & Cleaned)...")

    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∞ –Ω–∞–≤—ã–∫–æ–≤
    skill_map = load_skills(DATA_DIR / "skills.csv")

    # 2. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    df = load_merged_data(DATA_DIR)
    if df.empty:
        print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
        return

    print(f"   –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {len(df)}")

    # 3. –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞—Ç
    print("‚è≥ –†–∞—Å—á–µ—Ç –≤—Ä–µ–º–µ–Ω–∏ –∂–∏–∑–Ω–∏ –≤–∞–∫–∞–Ω—Å–∏–π...")
    df['upd_date'] = pd.to_datetime(df['last_update_date'], errors='coerce')
    df['pub_date'] = pd.to_datetime(df['publication_date'], errors='coerce')

    # –°—á–∏—Ç–∞–µ–º days_live (—Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º –∏ –ø—É–±–ª–∏–∫–∞—Ü–∏–µ–π)
    # –ï—Å–ª–∏ –¥–∞—Ç—ã —Å–æ–≤–ø–∞–¥–∞—é—Ç –∏–ª–∏ update –ø—É—Å—Ç–æ–π, —Å—Ç–∞–≤–∏–º –º–∏–Ω–∏–º—É–º 0.1 –¥–Ω—è, —á—Ç–æ–±—ã –Ω–µ –¥–µ–ª–∏—Ç—å –Ω–∞ –Ω–æ–ª—å
    df['days_live'] = (df['upd_date'] - df['pub_date']).dt.total_seconds() / (24 * 3600)
    df['days_live'] = df['days_live'].fillna(0).apply(lambda x: max(x, 0.1))

    # 4. –†–∞—Å—á–µ—Ç Velocity (–æ—Ç–∫–ª–∏–∫–∏ –≤ –¥–µ–Ω—å)
    df['total_responses'] = df['total_responses'].fillna(0)
    df['velocity'] = df['total_responses'] / df['days_live']

    # 5. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ Top Performers (–≠—Ç–∞–ª–æ–Ω–æ–≤)
    # –ë–µ—Ä–µ–º 80-–π –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å –ø–æ —Å–∫–æ—Ä–æ—Å—Ç–∏ –Ω–∞–±–æ—Ä–∞ –æ—Ç–∫–ª–∏–∫–æ–≤
    velocity_threshold = df['velocity'].quantile(0.8)

    # –£—Å–ª–æ–≤–∏–µ: –•–æ—Ä–æ—à–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –ò –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ (>300 —Å–∏–º–≤–æ–ª–æ–≤)
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –Ω–∞ —Å–ª—É—á–∞–π NaN
    df['vacancy_description'] = df['vacancy_description'].fillna("")

    df['is_top_performer'] = (df['velocity'] >= velocity_threshold) & (df['vacancy_description'].str.len() > 300)

    print(f"   üèÜ –ü–æ—Ä–æ–≥ Velocity (top 20%): {velocity_threshold:.2f}")
    print(f"   üåü –ù–∞–π–¥–µ–Ω–æ —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –≤–∞–∫–∞–Ω—Å–∏–π: {df['is_top_performer'].sum()}")

    # 6. –û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –∏ –º–∞–ø–ø–∏–Ω–≥ –Ω–∞–≤—ã–∫–æ–≤
    print("üßπ –û—á–∏—Å—Ç–∫–∞ HTML –∏ –º–∞–ø–ø–∏–Ω–≥ –Ω–∞–≤—ã–∫–æ–≤...")
    df['text_clean'] = df['vacancy_description'].apply(strip_tags)

    def map_ids_to_names(ids_str):
        ids = parse_pg_array(ids_str)
        # –ë–µ—Ä–µ–º –∏–º—è –∏–∑ –∫–∞—Ä—Ç—ã, –µ—Å–ª–∏ –Ω–µ—Ç - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
        names = [skill_map.get(i) for i in ids if i in skill_map]
        return ", ".join([n for n in names if n])

    if 'skill_ids' in df.columns:
        df['skills_str'] = df['skill_ids'].apply(map_ids_to_names)
    else:
        df['skills_str'] = ""

    # 7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    output_file = DATA_DIR / "vacancies_processed.parquet"
    final_cols = [
        'vacancy_id', 'vacancy_title', 'vacancy_description',
        'text_clean', 'skills_str', 'specialization',
        'profile', 'velocity', 'is_top_performer'
    ]

    # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ –∫–æ–ª–æ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ —Ä–µ–∞–ª—å–Ω–æ –µ—Å—Ç—å –≤ –¥–∞—Ç–∞—Ñ—Ä–µ–π–º–µ
    cols_to_save = [c for c in final_cols if c in df.columns]

    df[cols_to_save].to_parquet(output_file, index=False)
    print(f"‚úÖ –ì–æ—Ç–æ–≤–æ! –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_file}")


if __name__ == "__main__":
    main()
