import pandas as pd
import numpy as np
import pathlib

# --- –ù–ê–°–¢–†–û–ô–ö–ê –ü–£–¢–ï–ô ---
# –§–∞–π–ª –ª–µ–∂–∏—Ç –≤ src/data/prepare.py
CURRENT_DIR = pathlib.Path(__file__).resolve().parent
# –ü–æ–¥–Ω–∏–º–∞–µ–º—Å—è –Ω–∞ 2 —É—Ä–æ–≤–Ω—è: src/data -> src -> root
ROOT_DIR = CURRENT_DIR.parent.parent
DATA_DIR = ROOT_DIR / "data"

# –ü—É—Ç—å –∫ –∏—Å—Ö–æ–¥–Ω–æ–º—É CSV –∏ –∏—Ç–æ–≥–æ–≤–æ–º—É Parquet
RAW_FILE = DATA_DIR / "fact_vacancies_test.csv"
OUTPUT_FILE = DATA_DIR / "vacancies_processed.parquet"


def calculate_peak_efficiency(group, window_days=7):
    """
    –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –º–µ—Ç—Ä–∏–∫—É: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–∏—Ä–æ—Å—Ç –æ—Ç–∫–ª–∏–∫–æ–≤ –∑–∞ 7 –¥–Ω–µ–π.
    """
    group = group.sort_values('loaded_at')
    dates = group['loaded_at'].values
    responses = group['total_responses'].fillna(0).values

    n = len(group)
    if n < 2: return 0.0

    best_eff = 0.0

    for i in range(n):
        start_date = dates[i]
        limit_date = start_date + np.timedelta64(window_days, 'D')

        # –ò–Ω–¥–µ–∫—Å –∫–æ–Ω—Ü–∞ –æ–∫–Ω–∞
        end_idx = np.searchsorted(dates, limit_date, side='right') - 1

        if end_idx <= i: continue

        val_end = responses[end_idx]
        val_start = responses[i]

        current_eff = 0
        if val_start > 0:
            current_eff = val_end - val_start
        else:
            # –ï—Å–ª–∏ –Ω–∞—á–∞–ª–æ 0, –∏—â–µ–º –ø–µ—Ä–≤—ã–π >0
            window_slice = responses[i: end_idx + 1]
            nonzero_indices = np.nonzero(window_slice)[0]
            if len(nonzero_indices) > 0:
                first_nonzero = window_slice[nonzero_indices[0]]
                current_eff = val_end - first_nonzero
            else:
                current_eff = 0

        if current_eff > best_eff:
            best_eff = current_eff

    return float(best_eff)


def main():
    print(f"üöÄ –†–∞–±–æ—á–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {ROOT_DIR}")
    print(f"üìÇ –ò—â–µ–º —Ñ–∞–π–ª –¥–∞–Ω–Ω—ã—Ö: {RAW_FILE}")

    if not RAW_FILE.exists():
        print(f"‚ùå –û–®–ò–ë–ö–ê: –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω! –ü–æ–ª–æ–∂–∏—Ç–µ 'fact_vacancies_test.csv' –≤ –ø–∞–ø–∫—É 'data/'.")
        return

    df = pd.read_csv(RAW_FILE)
    df['loaded_at'] = pd.to_datetime(df['loaded_at'])

    print(f"üì¶ –ó–∞–≥—Ä—É–∂–µ–Ω–æ —Å—Ç—Ä–æ–∫: {len(df)}")

    # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫–∏
    print("‚è≥ –†–∞—Å—á–µ—Ç –ø–∏–∫–æ–≤–æ–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ (–º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è)...")
    best_vacancies = []

    for vac_id, group in df.groupby('vacancy_id'):
        eff = calculate_peak_efficiency(group)
        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –≤–µ—Ä—Å–∏—é –æ–ø–∏—Å–∞–Ω–∏—è
        best_row = group.sort_values('loaded_at').iloc[-1].to_dict()
        best_row['efficiency'] = eff
        best_vacancies.append(best_row)

    result_df = pd.DataFrame(best_vacancies)

    # –¢–æ–ø –ø–µ—Ä—Ñ–æ—Ä–º–µ—Ä—ã (Top 20%)
    threshold = result_df['efficiency'].quantile(0.8)
    result_df['is_top_performer'] = result_df['efficiency'] >= threshold

    print(f"üìä –ü–æ—Ä–æ–≥ (Top 20%): {threshold:.2f} –æ—Ç–∫–ª–∏–∫–æ–≤/–Ω–µ–¥–µ–ª—é")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
    cols = [
        'vacancy_id', 'profile', 'city', 'vacancy_title',
        'vacancy_description', 'specialization',
        'efficiency', 'is_top_performer'
    ]
    # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ, —á—Ç–æ –µ—Å—Ç—å –≤ df
    save_cols = [c for c in cols if c in result_df.columns]

    result_df[save_cols].to_parquet(OUTPUT_FILE, index=False)
    print(f"‚úÖ –ì–æ—Ç–æ–≤–æ! –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {OUTPUT_FILE}")


if __name__ == "__main__":
    main()
