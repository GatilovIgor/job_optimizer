import pandas as pd
import pathlib
from qdrant_client import QdrantClient
from qdrant_client.models import Distance, VectorParams, PointStruct
from sentence_transformers import SentenceTransformer
from typing import List, Dict


class VacancyRetriever:
    def __init__(self,
                 data_path: str = None,
                 model_name: str = "cointegrated/rubert-tiny2",  # –õ–µ–≥–∫–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ
                 collection_name: str = "vacancies_mvp"):

        self.model = SentenceTransformer(model_name)
        self.collection_name = collection_name

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π Qdrant (–≤ –ø–∞–º—è—Ç–∏), —á—Ç–æ–±—ã –Ω–µ –ø–æ–¥–Ω–∏–º–∞—Ç—å Docker
        # –î–ª—è –ø—Ä–æ–¥–∞–∫—à–µ–Ω–∞ –∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ URL —Å–µ—Ä–≤–µ—Ä–∞
        self.client = QdrantClient(":memory:")

        if data_path:
            self._index_data(data_path)

    def _index_data(self, data_path: str):
        print(f"üì• Loading data from {data_path}...")
        df = pd.read_parquet(data_path)

        # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ "–ß–µ–º–ø–∏–æ–Ω–æ–≤", —á—Ç–æ–±—ã —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞—Ç—å –ª—É—á—à–µ–µ
        top_df = df[df['is_top_performer'] == True].copy()
        print(f"   Indexing {len(top_df)} top performers...")

        # –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
        vectors = self.model.encode(top_df['text_clean'].tolist(), show_progress_bar=True)

        # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é
        self.client.recreate_collection(
            collection_name=self.collection_name,
            vectors_config=VectorParams(size=self.model.get_sentence_embedding_dimension(), distance=Distance.COSINE),
        )

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ—á–∫–∏
        points = []
        for i, row in top_df.reset_index().iterrows():
            payload = {
                "title": row['vacancy_title'],
                "velocity": row['velocity']
            }
            points.append(PointStruct(id=i, vector=vectors[i], payload=payload))

        self.client.upload_points(
            collection_name=self.collection_name,
            points=points
        )
        print("‚úÖ Indexing complete!")

    def search(self, query: str, limit: int = 3) -> List[Dict]:
        query_vector = self.model.encode(query).tolist()

        hits = self.client.search(
            collection_name=self.collection_name,
            query_vector=query_vector,
            limit=limit
        )

        return [
            {
                "title": hit.payload['title'],
                "velocity": hit.payload['velocity'],
                "score": hit.score
            }
            for hit in hits
        ]


# --- DEMO RUN ---
if __name__ == "__main__":
    root = pathlib.Path(__file__).resolve().parent.parent.parent
    data_file = root / "dataset" / "vacancies_processed.parquet"

    # 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è
    retriever = VacancyRetriever(data_path=str(data_file))

    # 2. –¢–µ—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫
    query = "–ò—â–µ–º –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –ø–æ –ø—Ä–æ–¥–∞–∂–∞–º"
    print(f"\nüîç Searching for: '{query}'...")

    results = retriever.search(query)

    for r in results:
        print(f"   üèÜ Found: {r['title']} (Speed: {r['velocity']:.1f}/day, Score: {r['score']:.2f})")
