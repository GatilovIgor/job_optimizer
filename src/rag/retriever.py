import pandas as pd
import pathlib
import pickle
import os
from sentence_transformers import SentenceTransformer
from sklearn.neighbors import NearestNeighbors
from typing import List, Dict


class VacancyRetriever:
    def __init__(self,
                 data_path: str = None,
                 model_name: str = "cointegrated/rubert-tiny2",
                 # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –Ω–∞–º –±–æ–ª—å—à–µ –Ω–µ –Ω—É–∂–Ω—ã, –Ω–æ –æ—Å—Ç–∞–≤–∏–º –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
                 collection_name: str = "vacancies_mvp",
                 force_reindex: bool = False):

        self.root = pathlib.Path(__file__).resolve().parent.parent.parent
        self.index_path = self.root / "dataset" / "vector_index.pkl"

        self.model = SentenceTransformer(model_name)

        self.index = None
        self.vacancies = []

        # –õ–æ–≥–∏–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏:
        # –ï—Å–ª–∏ –µ—Å—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å –∏ force=False -> –≥—Ä—É–∑–∏–º.
        # –ò–Ω–∞—á–µ -> —Å—Ç—Ä–æ–∏–º –∑–∞–Ω–æ–≤–æ.

        if not force_reindex and self.index_path.exists():
            print(f"‚úÖ Loading vector index from {self.index_path}...")
            with open(self.index_path, "rb") as f:
                saved_data = pickle.load(f)
                self.index = saved_data["index"]
                self.vacancies = saved_data["vacancies"]
        elif data_path:
            print("‚öôÔ∏è Building new vector index (sklearn)...")
            self._build_index(data_path)
        else:
            print("‚ö†Ô∏è No index found and no data path provided.")

    def _build_index(self, data_path: str):
        print(f"üì• Loading data from {data_path}...")
        df = pd.read_parquet(data_path)

        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ª—É—á—à–∏—Ö
        top_df = df[df['is_top_performer'] == True].copy().reset_index(drop=True)
        print(f"   Vectorizing {len(top_df)} vacancies...")

        # 1. –í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
        vectors = self.model.encode(top_df['text_clean'].tolist(), show_progress_bar=True)

        # 2. –°—Ç—Ä–æ–∏–º –∏–Ω–¥–µ–∫—Å (Brute force –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏, Metric=Cosine)
        # Cosine distance = 1 - Cosine Similarity
        index = NearestNeighbors(n_neighbors=10, metric="cosine", algorithm="brute")
        index.fit(vectors)

        # 3. –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ (–Ω–∞–º –Ω—É–∂–Ω—ã —Å–∞–º–∏ —Ç–µ–∫—Å—Ç—ã, —á—Ç–æ–±—ã –∏—Ö –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å)
        self.index = index
        self.vacancies = top_df.to_dict("records")

        # 4. –ü–∏—à–µ–º –Ω–∞ –¥–∏—Å–∫
        with open(self.index_path, "wb") as f:
            pickle.dump({
                "index": self.index,
                "vacancies": self.vacancies
            }, f)

        print("‚úÖ Index built and saved!")

    def search(self, query: str, limit: int = 3) -> List[Dict]:
        if not self.index:
            return []

        # –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º –∑–∞–ø—Ä–æ—Å
        query_vector = self.model.encode([query])

        # –ò—â–µ–º (–≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç distances –∏ indices)
        distances, indices = self.index.kneighbors(query_vector, n_neighbors=limit)

        results = []
        for i, idx in enumerate(indices[0]):
            # distance - —ç—Ç–æ –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ (0..2).
            # –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º –≤ similarity (1..-1) –¥–ª—è –∫—Ä–∞—Å–æ—Ç—ã
            score = 1 - distances[0][i]
            vac = self.vacancies[idx]

            results.append({
                "title": vac['vacancy_title'],
                "velocity": vac['velocity'],
                "score": float(score)
            })

        return results


if __name__ == "__main__":
    data_file = pathlib.Path(__file__).resolve().parent.parent.parent / "dataset" / "vacancies_processed.parquet"
    retriever = VacancyRetriever(data_path=str(data_file))
    print(retriever.search("Python"))
