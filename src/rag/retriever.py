import pandas as pd
import pathlib
import pickle
from sentence_transformers import SentenceTransformer
from sklearn.neighbors import NearestNeighbors
from typing import List, Dict


class VacancyRetriever:
    def __init__(self,
                 data_path: str = None,
                 model_name: str = "cointegrated/rubert-tiny2",
                 force_reindex: bool = False):

        # –ü—É—Ç—å –∫ –∫–æ—Ä–Ω—é –ø—Ä–æ–µ–∫—Ç–∞ (—Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –æ—Ç —Ç–µ–∫—É—â–µ–≥–æ —Ñ–∞–π–ª–∞)
        self.root = pathlib.Path(__file__).resolve().parent.parent.parent
        self.index_path = self.root / "dataset" / "vector_index.pkl"

        self.model = SentenceTransformer(model_name)

        self.index = None
        self.vacancies = []

        # –õ–æ–≥–∏–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏:
        # –ï—Å–ª–∏ –µ—Å—Ç—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–π –∏–Ω–¥–µ–∫—Å –∏ force=False -> –≥—Ä—É–∑–∏–º.
        # –ò–Ω–∞—á–µ -> —Å—Ç—Ä–æ–∏–º –∑–∞–Ω–æ–≤–æ –∏–∑ Parquet.

        if not force_reindex and self.index_path.exists():
            print(f"‚úÖ Loading vector index from {self.index_path}...")
            with open(self.index_path, "rb") as f:
                saved_data = pickle.load(f)
                self.index = saved_data["index"]
                self.vacancies = saved_data["vacancies"]
        elif data_path:
            print("‚öôÔ∏è Building new vector index (sklearn)...")
            self._build_index(data_path)
        else:
            print("‚ö†Ô∏è No index found and no data path provided.")

    def _build_index(self, data_path: str):
        print(f"üì• Loading data from {data_path}...")
        df = pd.read_parquet(data_path)

        # 1. –§–∏–ª—å—Ç—Ä: –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —É—Å–ø–µ—à–Ω—ã–µ –≤–∞–∫–∞–Ω—Å–∏–∏
        if 'is_top_performer' in df.columns:
            top_df = df[df['is_top_performer'] == True].copy().reset_index(drop=True)
            print(f"   Filtering: {len(df)} -> {len(top_df)} top performers.")
        else:
            print("‚ö†Ô∏è 'is_top_performer' column missing. Using all data.")
            top_df = df.copy()

        if len(top_df) == 0:
            print("‚ùå No vacancies found for indexing!")
            return

        # 2. –§–æ—Ä–º–∏—Ä—É–µ–º "Rich Embedding Context" (–ë–æ–≥–∞—Ç—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç)
        # –í–µ–∫—Ç–æ—Ä –¥–æ–ª–∂–µ–Ω —É—á–∏—Ç—ã–≤–∞—Ç—å –ø—Ä–æ—Ñ–µ—Å—Å–∏—é –∏ –Ω–∞–≤—ã–∫–∏, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ –æ–ø–∏—Å–∞–Ω–∏–µ.
        # –§–æ—Ä–º–∞—Ç: "–ó–∞–≥–æ–ª–æ–≤–æ–∫. –ü—Ä–æ—Ñ–∏–ª—å. –ù–∞–≤—ã–∫–∏. –¢–µ–∫—Å—Ç..."
        top_df['embedding_text'] = (
                top_df['vacancy_title'].fillna('') + ". " +
                top_df['specialization'].fillna('') + ". " +
                top_df['skills_str'].fillna('') + ". " +
                top_df['text_clean'].fillna('')
        )

        # –û–±—Ä–µ–∑–∞–µ–º —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã (–º–æ–¥–µ–ª—å —Å–∞–º–∞ –æ–±—Ä–µ–∂–µ—Ç, –Ω–æ –ª—É—á—à–µ –∑–∞—Ä–∞–Ω–µ–µ)
        top_df['embedding_text'] = top_df['embedding_text'].str.slice(0, 2000)

        print(f"   Vectorizing {len(top_df)} items...")
        vectors = self.model.encode(top_df['embedding_text'].tolist(), show_progress_bar=True)

        # 3. –°—Ç—Ä–æ–∏–º –∏–Ω–¥–µ–∫—Å (Brute force + Cosine)
        index = NearestNeighbors(n_neighbors=10, metric="cosine", algorithm="brute")
        index.fit(vectors)

        self.index = index
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ª–æ–≤–∞—Ä—å –∑–∞–ø–∏—Å–µ–π, —á—Ç–æ–±—ã –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –∏—Ö LLM
        # –í–ê–ñ–ù–û: —Å–æ—Ö—Ä–∞–Ω—è–µ–º 'vacancy_description' (HTML), –∞ –Ω–µ text_clean
        self.vacancies = top_df.to_dict("records")

        # 4. –ü–∏—à–µ–º –Ω–∞ –¥–∏—Å–∫
        with open(self.index_path, "wb") as f:
            pickle.dump({
                "index": self.index,
                "vacancies": self.vacancies
            }, f)

        print("‚úÖ Index built and saved!")

    def search(self, query: str, limit: int = 3) -> List[Dict]:
        if not self.index:
            return []

        # –í–µ–∫—Ç–æ—Ä–∏–∑—É–µ–º –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        query_vector = self.model.encode([query])

        # –ò—â–µ–º
        distances, indices = self.index.kneighbors(query_vector, n_neighbors=limit)

        results = []
        for i, idx in enumerate(indices[0]):
            vac = self.vacancies[idx]

            # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ (0..2) -> –°—Ö–æ–¥—Å—Ç–≤–æ (1..-1)
            score = 1 - distances[0][i]

            results.append({
                "title": vac['vacancy_title'],
                # –û—Ç–¥–∞–µ–º LLM –∏—Å—Ö–æ–¥–Ω—ã–π HTML –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä–µ
                "html_text": vac.get('vacancy_description', vac.get('text_clean', '')),
                "velocity": vac.get('velocity', 0.0),
                "score": float(score)
            })

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞: —Å–Ω–∞—á–∞–ª–∞ —Å–∞–º—ã–µ –ø–æ—Ö–æ–∂–∏–µ (score), –ø—Ä–∏ —Ä–∞–≤–µ–Ω—Å—Ç–≤–µ - —Å–∞–º—ã–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ (velocity)
        results.sort(key=lambda x: (x['score'], x['velocity']), reverse=True)
        return results
