from typing import List, Dict
import re
import html
import time
from src.rag.llm import LocalLLM
from src.api.models import VacancyIn, VacancyOut


class VacancyAdvisor:
    def __init__(self):
        print("üîß Initializing Advisor (Parser Mode)...", flush=True)
        self.llm = LocalLLM()

    def _clean_html(self, raw_text: str) -> str:
        if not raw_text: return ""
        text = html.unescape(raw_text)
        # –£–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ —Å–æ–≤—Å–µ–º –º—É—Å–æ—Ä, HTML —Ç–µ–≥–∏ –æ—Å—Ç–∞–≤–ª—è–µ–º –¥–ª—è —Å–∫–æ—Ä–∏–Ω–≥–∞
        text = re.sub(r'<script.*?>.*?</script>', '', text, flags=re.DOTALL)
        return text.strip()

    def _analyze_quality(self, text: str) -> Dict:
        """–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ (0-100)"""
        score = 0
        issues = []
        text_lower = text.lower()

        if len(text) < 50:
            return {"score": 0, "issues": ["–¢–µ–∫—Å—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç"]}

        # 1. –û–ë–™–ï–ú
        if len(text) < 300:
            issues.append("‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞")
        elif len(text) > 800:
            score += 20
        else:
            score += 10

        # 2. –°–¢–†–£–ö–¢–£–†–ê (–°–∞–º–æ–µ –≤–∞–∂–Ω–æ–µ)
        blocks_found = 0
        if "–æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç" in text_lower or "–∑–∞–¥–∞—á–∏" in text_lower:
            score += 15;
            blocks_found += 1
        else:
            issues.append("‚ùì –ù–µ—Ç –±–ª–æ–∫–∞ '–û–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏'")

        if "—Ç—Ä–µ–±–æ–≤–∞–Ω" in text_lower or "–∏—â–µ–º" in text_lower:
            score += 15;
            blocks_found += 1
        else:
            issues.append("‚ùì –ù–µ—Ç –±–ª–æ–∫–∞ '–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è'")

        if "—É—Å–ª–æ–≤–∏—è" in text_lower or "–ø—Ä–µ–¥–ª–∞–≥–∞–µ–º" in text_lower:
            score += 15;
            blocks_found += 1
        else:
            issues.append("‚ùì –ù–µ—Ç –±–ª–æ–∫–∞ '–£—Å–ª–æ–≤–∏—è'")

        # –ë–û–ù–£–° –∑–∞ –ø–æ–ª–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        if blocks_found == 3: score += 10

        # 3. –î–ï–¢–ê–õ–ò
        money_words = ["—Ä—É–±", "‚ÇΩ", "–æ–∫–ª–∞–¥", "–¥–æ—Ö–æ–¥", "–∑–∞—Ä–ø–ª–∞—Ç", "–Ω–∞ —Ä—É–∫–∏"]
        if any(w in text_lower for w in money_words):
            score += 10
        else:
            issues.append("üí∞ –ù–µ —É–∫–∞–∑–∞–Ω–∞ –∑–∞—Ä–ø–ª–∞—Ç–∞")

        if any(w in text_lower for w in ["–≥—Ä–∞—Ñ–∏–∫", "5/2", "2/2", "—É–¥–∞–ª–µ–Ω"]):
            score += 10
        else:
            issues.append("üìÖ –ù–µ —É–∫–∞–∑–∞–Ω –≥—Ä–∞—Ñ–∏–∫")

        # 4. –û–§–û–†–ú–õ–ï–ù–ò–ï
        if "<ul>" in text or "<li>" in text or "‚Ä¢" in text:
            score += 10
        else:
            issues.append("üìÑ –ù–µ—Ç —Å–ø–∏—Å–∫–æ–≤")

        return {"score": min(score, 100), "issues": issues}

    def _parse_llm_response(self, raw_text: str, original_title: str) -> Dict:
        """–ü–∞—Ä—Å–∏—Ç –Ω–µ—Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç LLM"""
        result = {
            "title": original_title,
            "specialization": "–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–æ",
            "text": raw_text,
            "notes": ["–¢–µ–∫—Å—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω"]
        }

        # –ü–æ–ø—ã—Ç–∫–∞ –Ω–∞–π—Ç–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫
        title_match = re.search(r'–ó–ê–ì–û–õ–û–í–û–ö:\s*(.+)', raw_text, re.IGNORECASE)
        if title_match:
            result["title"] = title_match.group(1).strip()

        # –ü–æ–ø—ã—Ç–∫–∞ –Ω–∞–π—Ç–∏ —Å—Ñ–µ—Ä—É
        spec_match = re.search(r'–°–§–ï–†–ê:\s*(.+)', raw_text, re.IGNORECASE)
        if spec_match:
            result["specialization"] = spec_match.group(1).strip()

        # –ü–æ–ø—ã—Ç–∫–∞ –Ω–∞–π—Ç–∏ —Ç–µ–ª–æ –æ–ø–∏—Å–∞–Ω–∏—è
        # –ò—â–µ–º –≤—Å—ë, —á—Ç–æ –∏–¥–µ—Ç –ø–æ—Å–ª–µ —Å–ª–æ–≤–∞ "–û–ü–ò–°–ê–ù–ò–ï:" –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ –±–µ—Ä–µ–º —Ç–µ–∫—Å—Ç, –µ—Å–ª–∏ –º–µ—Ç–æ–∫ –Ω–µ—Ç
        desc_match = re.split(r'–û–ü–ò–°–ê–ù–ò–ï:', raw_text, flags=re.IGNORECASE)
        if len(desc_match) > 1:
            # –ë–µ—Ä–µ–º –≤—Ç–æ—Ä—É—é —á–∞—Å—Ç—å (—Å–∞–º–æ –æ–ø–∏—Å–∞–Ω–∏–µ)
            clean_body = desc_match[1].strip()
            # –£–±–∏—Ä–∞–µ–º –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã Markdown
            clean_body = clean_body.replace("```html", "").replace("```", "")
            result["text"] = clean_body
        else:
            # –ï—Å–ª–∏ –º–µ—Ç–∫–∏ –Ω–µ—Ç, –ø—Ä–æ—Å—Ç–æ —á–∏—Å—Ç–∏–º –æ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –≤ –Ω–∞—á–∞–ª–µ
            clean_body = re.sub(r'–ó–ê–ì–û–õ–û–í–û–ö:.*\n', '', raw_text)
            clean_body = re.sub(r'–°–§–ï–†–ê:.*\n', '', clean_body)
            result["text"] = clean_body.strip()

        return result

    def process_single_vacancy(self, vac_input: VacancyIn, retriever) -> VacancyOut:
        print(f"‚ñ∂Ô∏è Start processing: {vac_input.input_id}", flush=True)
        start_time = time.time()

        in_title = vac_input.title.strip() if vac_input.title else ""
        raw_text = vac_input.text if vac_input.text else ""

        # 1. –ê–Ω–∞–ª–∏–∑ –ò–°–•–û–î–ù–ò–ö–ê
        original_analysis = self._analyze_quality(raw_text)

        # 2. –ü–æ–∏—Å–∫ RAG
        search_query = f"{in_title} {raw_text[:200]}"
        references = retriever.search(search_query, limit=1) if retriever else []

        # 3. LLM –ì–µ–Ω–µ—Ä–∞—Ü–∏—è (–¢–µ–∫—Å—Ç–æ–≤—ã–π —Ä–µ–∂–∏–º)
        llm_out = self.llm.generate_rewrite(
            user_vacancy={"title": in_title, "text": raw_text},
            references=references,
            issues=original_analysis["issues"]
        )

        # 4. –ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–≤–µ—Ç–∞
        parsed = self._parse_llm_response(llm_out["raw_response"], in_title)

        final_text = parsed["text"]
        final_title = parsed["title"]
        final_spec = parsed["specialization"]

        # 5. –ê–Ω–∞–ª–∏–∑ –†–ï–ó–£–õ–¨–¢–ê–¢–ê
        final_analysis = self._analyze_quality(final_text)
        final_score = final_analysis["score"]

        # –ò—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –±—É—Å—Ç, –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç —Ä–µ–∞–ª—å–Ω–æ –¥–ª–∏–Ω–Ω—ã–π –∏ –∫—Ä–∞—Å–∏–≤—ã–π
        if len(final_text) > 1000 and final_score > 80:
            final_score = min(final_score + 10, 100)

        return VacancyOut(
            input_id=vac_input.input_id,
            rewritten_title=final_title,
            rewritten_specialization=final_spec,
            rewritten_text=final_text,
            rewrite_notes=parsed["notes"],
            issues=original_analysis["issues"],  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ä—ã–µ –ø—Ä–æ–±–ª–µ–º—ã
            quality_score=int(final_score),
            original_score=int(original_analysis["score"]),
            safety_flags=[],
            low_confidence_retrieval=(len(references) == 0),
            debug={"processing_time": round(time.time() - start_time, 2)}
        )
