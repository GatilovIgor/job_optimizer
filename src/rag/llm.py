import os
import re
from huggingface_hub import hf_hub_download

try:
    from llama_cpp import Llama
except ImportError:
    Llama = None

from transformers import logging as transformers_logging

transformers_logging.set_verbosity_error()
os.environ["HF_XET_HIGH_PERFORMANCE"] = "0"


class LocalLLM:
    def __init__(self,
                 repo_id="Qwen/Qwen2.5-1.5B-Instruct-GGUF",
                 filename="qwen2.5-1.5b-instruct-q4_k_m.gguf",
                 n_ctx=2048):

        if Llama is None:
            raise ImportError("Please install llama-cpp-python")

        print(f"üì¶ checking model: {filename}...", flush=True)
        try:
            model_path = hf_hub_download(repo_id=repo_id, filename=filename)

            self.llm = Llama(
                model_path=model_path,
                n_ctx=n_ctx,
                n_gpu_layers=-1,
                n_threads=6,
                verbose=False
            )
            print("üì¶ Model loaded!", flush=True)
        except Exception as e:
            print(f"‚ùå Failed to load LLM: {e}")
            self.llm = None

    def generate_rewrite(self, user_vacancy: dict, references: list, issues: list) -> dict:
        if not self.llm:
            return {"raw_response": "–û—à–∏–±–∫–∞: –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞"}

        print("      [LLM] Generating (Text Mode)...", flush=True)

        # –ö–æ–Ω—Ç–µ–∫—Å—Ç
        ref_text = ""
        if references:
            ref_content = references[0].get('html_text', '')[:800]
            ref_text = f"–ü–†–ò–ú–ï–† (–°–¢–ò–õ–¨):\n{ref_content}..."

        issues_str = ", ".join(issues) if issues else "–£–ª—É—á—à–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏ –ø—Ä–æ–¥–∞—é—â–∏–π —Å—Ç–∏–ª—å."
        title = user_vacancy.get('title', '–°–æ—Ç—Ä—É–¥–Ω–∏–∫')
        text = user_vacancy.get('text', '')

        if len(text) < 100:
            text += "\n(–≠—Ç–æ —á–µ—Ä–Ω–æ–≤–∏–∫. –ü—Ä–∏–¥—É–º–∞–π –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Å –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç—è–º–∏, —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏ –∏ —É—Å–ª–æ–≤–∏—è–º–∏.)"

        # –ü–†–û–ú–ü–¢ –ë–ï–ó JSON (–ü—Ä–æ—Å–∏–º –ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç)
        system_prompt = (
            "–¢—ã ‚Äî –æ–ø—ã—Ç–Ω—ã–π HR-—Ä–µ–¥–∞–∫—Ç–æ—Ä. –ù–∞–ø–∏—à–∏ –õ–£–ß–®–ï–ï –æ–ø–∏—Å–∞–Ω–∏–µ –≤–∞–∫–∞–Ω—Å–∏–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.\n"
            "–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –°–¢–†–û–ì–û —Ç–∞–∫–æ–π:\n"
            "–ó–ê–ì–û–õ–û–í–û–ö: [–ù–∞–∑–≤–∞–Ω–∏–µ –¥–æ–ª–∂–Ω–æ—Å—Ç–∏]\n"
            "–°–§–ï–†–ê: [–°—Ñ–µ—Ä–∞ –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏]\n"
            "–û–ü–ò–°–ê–ù–ò–ï:\n"
            "[–í—Å—Ç—É–ø–ª–µ–Ω–∏–µ]\n"
            "<h3>–û–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏:</h3>\n<ul><li>...</li></ul>\n"
            "<h3>–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:</h3>\n<ul><li>...</li></ul>\n"
            "<h3>–£—Å–ª–æ–≤–∏—è:</h3>\n<ul><li>...</li></ul>"
        )

        user_message = (
            f"{ref_text}\n\n"
            f"–ó–ê–î–ê–ß–ê: –ò—Å–ø—Ä–∞–≤—å –∏ –¥–æ–ø–æ–ª–Ω–∏ –≤–∞–∫–∞–Ω—Å–∏—é.\n"
            f"–¢–µ–∫—É—â–∞—è –¥–æ–ª–∂–Ω–æ—Å—Ç—å: {title}\n"
            f"–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: {text}\n"
            f"–ß—Ç–æ –∏—Å–ø—Ä–∞–≤–∏—Ç—å: {issues_str}\n\n"
            "–ù–∞—á–∏–Ω–∞–π –æ—Ç–≤–µ—Ç —Å—Ä–∞–∑—É —Å –ø–æ–ª—è '–ó–ê–ì–û–õ–û–í–û–ö:'."
        )

        try:
            response = self.llm.create_chat_completion(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_message}
                ],
                temperature=0.6,  # –ë–æ–ª—å—à–µ –∫—Ä–µ–∞—Ç–∏–≤–∞
                max_tokens=1600,
            )

            raw_text = response['choices'][0]['message']['content']
            return {"raw_response": raw_text}

        except Exception as e:
            print(f"      ‚ùå LLM Gen Error: {e}", flush=True)
            return {"raw_response": text}
