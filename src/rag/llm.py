import os
import pathlib

# –î–ª—è Windows –æ—Ç–∫–ª—é—á–∞–µ–º HF Transfer, –µ—Å–ª–∏ –æ–Ω –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "0"

from huggingface_hub import hf_hub_download
from llama_cpp import Llama


class LocalLLM:
    def __init__(self,
                 repo_id="Qwen/Qwen2.5-3B-Instruct-GGUF",
                 filename="qwen2.5-3b-instruct-q4_k_m.gguf",
                 n_ctx=4096):
        print(f"‚öôÔ∏è Initializing Local LLM ({repo_id})...")

        # 1. –°–∫–∞—á–∏–≤–∞–µ–º –º–æ–¥–µ–ª—å (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∫—ç—à–∏—Ä—É–µ—Ç—Å—è)
        model_path = hf_hub_download(
            repo_id=repo_id,
            filename=filename
        )
        print(f"   Model path: {model_path}")

        # 2. –ó–∞–≥—Ä—É–∂–∞–µ–º (CPU mode)
        self.llm = Llama(
            model_path=model_path,
            n_ctx=n_ctx,
            n_gpu_layers=0,  # 0 = CPU only
            verbose=False
        )

    def generate_advice(self, user_vacancy: str, references: list) -> str:
        # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        ref_text = ""
        for i, r in enumerate(references):
            ref_text += f"\n--- –ü–†–ò–ú–ï–† {i + 1} ---\n{r['title']}\n"

        system_prompt = (
            "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –Ω–∞–π–º—É. –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –¥–∞—Ç—å –∫—Ä–∞—Ç–∫–∏–µ —Å–æ–≤–µ—Ç—ã –ø–æ —É–ª—É—á—à–µ–Ω–∏—é –≤–∞–∫–∞–Ω—Å–∏–∏, "
            "—Å—Ä–∞–≤–Ω–∏–≤–∞—è –µ—ë —Å —É—Å–ø–µ—à–Ω—ã–º–∏ –ø—Ä–∏–º–µ—Ä–∞–º–∏."
        )

        user_message = (
            f"–ú–û–Ø –í–ê–ö–ê–ù–°–ò–Ø: {user_vacancy}\n\n"
            f"–£–°–ü–ï–®–ù–´–ï –ü–†–ò–ú–ï–†–´:{ref_text}\n\n"
            "–ù–∞–ø–∏—à–∏ 3 –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Å–æ–≤–µ—Ç–∞, –∫–∞–∫ —É–ª—É—á—à–∏—Ç—å –º–æ—é –≤–∞–∫–∞–Ω—Å–∏—é, —á—Ç–æ–±—ã –æ–Ω–∞ –±—ã–ª–∞ –ø–æ—Ö–æ–∂–∞ –Ω–∞ —É—Å–ø–µ—à–Ω—ã–µ. "
            "–û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ."
        )

        output = self.llm.create_chat_completion(
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_message}
            ],
            temperature=0.7,
            max_tokens=800
        )

        return output['choices'][0]['message']['content']


# --- TEST ---
if __name__ == "__main__":
    bot = LocalLLM()
    print("\nüí¨ Thinking...")
    res = bot.generate_advice("–ò—â–µ–º –ø–∏—Ç–æ–Ω–∏—Å—Ç–∞", [{"title": "Senior Python (Remote)"}])
    print("\nüí° RESULT:\n", res)
