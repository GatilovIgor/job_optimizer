import os
import pathlib

try:
    from llama_cpp import Llama
except ImportError:
    Llama = None

class LocalLLM:
    def __init__(self, n_ctx=4096):
        if Llama is None:
            raise ImportError("Please install llama-cpp-python")

        # 1. –ù–∞—Ö–æ–¥–∏–º –ø—É—Ç—å –∫ –º–æ–¥–µ–ª–∏
        # –ü–æ–¥–Ω–∏–º–∞–µ–º—Å—è –æ—Ç src/rag/llm.py –¥–æ –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞
        root_dir = pathlib.Path(__file__).resolve().parent.parent.parent
        model_path = root_dir / "models" / "Llama-3.2-3B-Instruct-Q4_K_M.gguf"

        print(f"üì¶ Loading local model: {model_path}...", flush=True)

        if not model_path.exists():
            raise FileNotFoundError(f"‚ùå Model file not found at: {model_path}\nPlease put the .gguf file in the 'models' folder.")

        try:
            self.llm = Llama(
                model_path=str(model_path), # –ü—É—Ç—å –∫ –≤–∞—à–µ–º—É —Ñ–∞–π–ª—É
                n_ctx=n_ctx,
                n_gpu_layers=-1,
                n_threads=6,
                verbose=False
            )
            print("üì¶ Llama-3.2-3B Loaded!", flush=True)
        except Exception as e:
            print(f"‚ùå Failed to load LLM: {e}")
            self.llm = None

    def generate_rewrite(self, user_vacancy: dict, references: list, issues: list) -> dict:
        if not self.llm:
            return {"raw_response": "–û—à–∏–±–∫–∞: –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞"}

        print("      [Llama] Generating...", flush=True)

        title = user_vacancy.get('title', '–°–æ—Ç—Ä—É–¥–Ω–∏–∫')
        text = user_vacancy.get('text', '')

        if len(text) < 100:
            text += f"\n(–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è —Å–∫—É–ø–∞—è. –ü—Ä–∏–¥—É–º–∞–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–µ –æ–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏ –∏ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –¥–ª—è —Ä–æ–ª–∏ '{title}')"

        system_prompt = (
            "You are a professional HR Specialist. Output MUST be in Russian.\n"
            "Follow the structure exactly."
        )

        user_message = (
            f"–ù–∞–ø–∏—à–∏ –ø–æ–¥—Ä–æ–±–Ω—É—é –≤–∞–∫–∞–Ω—Å–∏—é: {title}.\n"
            f"–ß–µ—Ä–Ω–æ–≤–∏–∫: {text}\n\n"
            "–°–¢–†–£–ö–¢–£–†–ê –û–¢–í–ï–¢–ê:\n"
            "–ó–ê–ì–û–õ–û–í–û–ö: [–î–æ–ª–∂–Ω–æ—Å—Ç—å]\n"
            "–°–§–ï–†–ê: [–°—Ñ–µ—Ä–∞]\n"
            "–û–ü–ò–°–ê–ù–ò–ï:\n"
            "<p>[–í—Å—Ç—É–ø–ª–µ–Ω–∏–µ]</p>\n"
            "<h3>–û–±—è–∑–∞–Ω–Ω–æ—Å—Ç–∏:</h3>\n"
            "<ul>\n"
            "<li>[–ü—É–Ω–∫—Ç 1]</li>\n"
            "<li>[–ü—É–Ω–∫—Ç 2]</li>\n"
            "<li>[–ü—É–Ω–∫—Ç 3]</li>\n"
            "<li>[–ü—É–Ω–∫—Ç 4]</li>\n"
            "<li>[–ü—É–Ω–∫—Ç 5]</li>\n"
            "</ul>\n"
            "<h3>–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:</h3>\n"
            "<ul>\n"
            "<li>[–ü—É–Ω–∫—Ç 1]</li>\n"
            "<li>[–ü—É–Ω–∫—Ç 2]</li>\n"
            "<li>[–ü—É–Ω–∫—Ç 3]</li>\n"
            "</ul>\n"
            "<h3>–£—Å–ª–æ–≤–∏—è:</h3>\n"
            "<ul>\n"
            "<li>[–ó–∞—Ä–ø–ª–∞—Ç–∞]</li>\n"
            "<li>[–ì—Ä–∞—Ñ–∏–∫]</li>\n"
            "<li>[–û—Ñ–∏—Å/–ë–æ–Ω—É—Å—ã]</li>\n"
            "</ul>"
        )

        try:
            response = self.llm.create_chat_completion(
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_message}
                ],
                temperature=0.7,
                max_tokens=2000,
            )
            return {"raw_response": response['choices'][0]['message']['content']}
        except Exception as e:
            print(f"      ‚ùå LLM Gen Error: {e}", flush=True)
            return {"raw_response": text}
